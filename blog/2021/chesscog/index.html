<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> chess recognition | Georg Wölflein </title> <meta name="author" content="Georg Wölflein"> <meta name="description" content="parsing chess game state from a picture of the board"> <meta name="keywords" content="deep-learning, ai, ml, artificial-intelligence, machine-learning, digital-pathology"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%94%A5&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://georg.woelflein.eu/blog/2021/chesscog/"> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Georg</span> Wölflein </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Georg_Wolflein_CV.pdf" target="_blank"> cv </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">chess recognition</h1> <p class="post-meta"> Created on June 30, 2021 </p> <p class="post-tags"> <a href="/blog/2021"> <i class="fa-solid fa-calendar fa-sm"></i> 2021 </a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>If you’re a recreational chess player, the following scenario will likely sound familiar: you’re plaing a casual over-the-board game, and reach an interesting position. You may even be thinking about a <a href="https://www.youtube.com/watch?v=G90SVhxKeig" rel="external nofollow noopener" target="_blank">spicy piece sacrifice</a>. Long story short, you want to perform a computer analysis after the game, to see if you made the right decision. So, you take a picture of the current position before proceeding with the game.</p> <p>After the game, you can use this picture to enter the position into a chess analysis program, like the popular <a href="https://lichess.org/analysis" rel="external nofollow noopener" target="_blank">analysis board tool on Lichess</a>. However, this process is time-consuming and error-prone – you need to drag &amp; drop pieces around the board, until reaching the position you had in the photo.</p> <p>With all the recent (and not-so-recent) advances in deep learning and computer vision, you’d think it would be possible to automate this tedious procedure. Enter <em>chesscog</em>, an end-to-end chess recognition system I developed as part of my master’s thesis at the University of St Andrews and published as a journal article <a class="citation" href="#wolflein2021jimaging">[1]</a>. The goal of this project was to develop a system that is able to map a photo of a chess position to a structured format that can be understood by chess engines, such as the widely-used Forsyth-Edwards Notation (FEN). Perhaps the problem is best explained by a screenshot of the solution (a <a href="https://www.chesscog.com" rel="external nofollow noopener" target="_blank">demo app</a>) below.</p> <p><img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/demo_screenshot.png" data-zoomable=""></p> <div class="caption"> Screenshot of the <i>chesscog</i> <a href="https://www.chesscog.com" rel="external nofollow noopener" target="_blank">app</a>. </div> <p>How does it work? I’ll give you the short version in this blog post; if you’re interested, check out my <a href="https://github.com/georg-wolflein/chesscog-report/raw/master/report.pdf" rel="external nofollow noopener" target="_blank">master’s thesis</a>, the associated <a href="http://mdpi.com/2313-433X/7/6/94" rel="external nofollow noopener" target="_blank">paper</a>, and the <a href="https://github.com/georg-wolflein/chesscog" rel="external nofollow noopener" target="_blank">code</a> on GitHub.</p> <h1 id="method">Method</h1> <p>When you think about the problem, it seems logical to decompose it into three steps:</p> <ol> <li> <p><strong>Board localisation.</strong> First, we need to localise the board, i.e. determine the four corner points. This is done using traditional computer vision techniques such as Canny edge detection <a class="citation" href="#canny1986">[2]</a>, Hough transform <a class="citation" href="#duda1972">[3]</a>, and RANSAC <a class="citation" href="#fischler1981">[4]</a>.</p> <p>Once the corner points are established, we can perform some simple geometric calculations to compute the nine horizontal and vertical grid lines, taking into account perspective distortion (the camera parameters are <em>not</em> required for this). It actually turns out that it is more convenient to remove the perspective distortion by projecting the localised chessboard onto a regular square grid.</p> <div class="row"> <div class="col-sm-7 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/3828_corners_unwarped_result.png" data-zoomable=""> </div> <div class="col-sm-5 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/3828_corners_warped_result.png" data-zoomable=""> </div> </div> <div class="caption"> The perspective distortion is removed from the left image by projecting the four corner points onto a square grid (right). </div> </li> <li> <p><strong>Occupancy classification.</strong> Next, we can crop out each of the chess squares from the un-distorted image, and feed them into a convolutional neural network (CNN) acting as a binary classifier between empty and occupied squares. It turns out that performing this step prior to the piece classification significantly increases accuracy (as opposed to considering the empty square as a piece type).</p> <p><img class="img-fluid rounded z-depth-1 center" src="/assets/blog/chesscog/occupancy_convnet.png" data-zoomable=""></p> <div class="caption"> Example of an occupancy classification CNN that distinguishes between empty and occupied squares. </div> </li> <li> <p><strong>Piece classification.</strong> Finally, the occupied samples are fed through another (larger) CNN that performs a 12-way classification to determine piece colour (black or white), and piece type (pawn, knight, bishop, rook, queen, or king). The predictions are gathered for each of the 64 squares and used to generate the corresponding FEN string.</p> </li> </ol> <h1 id="training">Training</h1> <p>In order to train the CNNs, a dataset is required. The lack of sizable and adequately labelled datasets is an issue recognised by several prior works <a class="citation" href="#ding2016">[5], [6], [7]</a>. Instead of manually creating a dataset and labelling it, I modelled a chess set in <a href="http://blender.org" rel="external nofollow noopener" target="_blank">Blender</a>, and created a Python script render ~5,000 synthetic chessboard images with different camera poses, lighting conditions, and game states. This approach allowed me to create a much larger dataset for chess recognition – I made it publicly available <a href="http://osf.io/xf3ka/" rel="external nofollow noopener" target="_blank">here</a> <a class="citation" href="#wolflein2021">[8]</a>, to aid future research and enable fair comparison over a common dataset.</p> <p>I trained the occupancy classification and piece classification CNNs separately on the synthesised dataset. I tried out various hyperparameters and initialisations, and achieved the best results with models that were pre-traind on ImageNet. In the end, the system achieved an error rate of 0.23% per square on the test set, 28 times better than the previous state of the art <a class="citation" href="#mehta2020">[7]</a>.</p> <h1 id="adapting-to-unseen-chess-sets">Adapting to unseen chess sets</h1> <p>The last piece of the puzzle comes with the realisation that every chess set has a different appearance – different shapes, materials, colours, etc. The pipeline described above does not work very well out-of-the-box on unseen real-life chess sets because it was only trained on synthetic images. So, how do we adapt to an unseen chess set?</p> <p>At this point it is important to note that the first step in the pipeline <em>does</em> actually work quite well on unseen chess sets, due to the use of traditional computer vision techniques (as opposed to learned features). This means that we can actually perform the perspective unwarping, and extract the 64 squares from practically any input image of a chess set.</p> <p>This lead me to the idea to ask the user to supply two pictures to fine-tune the CNNs to any new chess set. These two pictures should be of the starting position, once each players perspective. Using the board localisation algorithm, it turns out that just these two images can generate enough training data to fine-tune the two CNNs (keep in mind that each image generates 64 samples of chess squares), although careful data augmentations were required to achieve good results. The CNNs were initialised with the trained weights from the synthesised dataset, and then fine-tuned using the newly cropped and augmented images.</p> <div class="row"> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/transfer_learning_white.png" data-zoomable=""> </div> <div class="col-sm-6 mt-3 mt-md-0"> <img class="img-fluid rounded z-depth-1" src="/assets/blog/chesscog/transfer_learning_black.png" data-zoomable=""> </div> </div> <div class="caption"> An example of the two input images required to fine-tune the CNNs. </div> <p>In the evaluation, the fine-tuning dataset consisted of the two images depicted above, and the test dataset contained 27 images obtained by playing a game of chess and taking a picture after each move. Even on this unseen dataset, the chess recognition pipeline achieved very convincing results. The per-square misclassification rate was just 0.17% on the test set. See the table below for a summary of the results.</p> <p><img class="img-fluid rounded z-depth-1 center" src="/assets/blog/chesscog/results.png" data-zoomable=""></p> <div class="caption"> Performance of the chess recognition pipeline. </div> <h1 id="whats-next">What’s next?</h1> <p>There are plenty avenues to continue this research, below I’ll name a few:</p> <ul> <li>Improve the process of adapting to unseen chess sets, for example by employing <a href="https://arxiv.org/abs/1703.03400" rel="external nofollow noopener" target="_blank">meta-learning</a> to reduce the number of steps needed to fine-tune the network.</li> <li>Train on a more diverse dataset of real-world images, to eliminate the need for fine-tuning entirely.</li> <li>Devise a differentiable pipeline that can be trained end-to-end (as opposed to the three steps outlined in this blog post which are trained separately).</li> <li>Improve the <a href="https://www.chesscog.com" rel="external nofollow noopener" target="_blank">web app</a> (it currently runs in <a href="http://heroku.com" rel="external nofollow noopener" target="_blank">Heroku</a>’s free tier on a single CPU, frequently running into out-of-memory errors).</li> </ul> <p>If you’re keen on helping on any of these points, or have other suggestions, drop me an email!</p> <h1 id="further-links">Further links</h1> <ul> <li>The <em>chesscog</em> <a href="https://www.chesscog.com" rel="external nofollow noopener" target="_blank">app</a>.</li> <li>The <a href="https://github.com/georg-wolflein/chesscog" rel="external nofollow noopener" target="_blank">code</a> is available on GitHub.</li> <li>My <a href="http://mdpi.com/2313-433X/7/6/94" rel="external nofollow noopener" target="_blank">article</a> in the <em>Journal of Imaging</em>, “Determining Chess Game State From an Image”.</li> <li>My <a href="https://github.com/georg-wolflein/chesscog-report/raw/master/report.pdf" rel="external nofollow noopener" target="_blank">master’s thesis</a>, which goes into more detail than this blog post and the journal article.</li> </ul> </div> </article> <h2>References</h2> <ol class="bibliography"> <li> <span id="wolflein2021jimaging">G. Wölflein and O. Arandjelović, “Determining Chess Game State from an Image,” <i>Journal of Imaging</i>, vol. 7, no. 6, 2021.</span> </li> <li> <span id="canny1986">J. Canny, “A Computational Approach to Edge Detection,” <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>, 1986.</span> </li> <li> <span id="duda1972">R. O. Duda and P. E. Hart, “Use of the Hough Transformation to Detect Lines and Curves in Pictures,” <i>Communications of the ACM</i>, vol. 15, no. 1, 1972.</span> </li> <li> <span id="fischler1981">M. A. Fischler and R. C. Bolles, “Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography,” <i>Communications of the ACM</i>, vol. 24, no. 6, 1981.</span> </li> <li> <span id="ding2016">J. Ding, “ChessVision: Chess Board and Piece Recognition.” Stanford University, 2016.</span> </li> <li> <span id="czyzewski2020">M. A. Czyzewski, A. Laskowski, and S. Wasik, “Chessboard and Chess Piece Recognition with the Support of Neural Networks.” 2020.</span> </li> <li> <span id="mehta2020">A. Mehta and H. Mehta, “Augmented Reality Chess Analyzer (ARChessAnalyzer),” <i>Journal of Emerging Investigators</i>, vol. 2, 2020.</span> </li> <li> <span id="wolflein2021">G. Wölflein and O. Arandjelović, “Dataset of Rendered Chess Game State Images.” Open Science Framework, 2021. doi: 10.17605/OSF.IO/XF3KA.</span> </li> </ol> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Georg Wölflein. Powered by <a href="http://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with the <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="external nofollow noopener">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>